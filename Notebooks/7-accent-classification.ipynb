{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwWiYfuMiTQE"
      },
      "source": [
        "### Installs, imports and settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Asam2Dzy4HlU"
      },
      "outputs": [],
      "source": [
        "# Datasets:\n",
        "DATASET = 'n-f-n'\n",
        "# DATASET = 's-f-o'\n",
        "# DATASET = 's-f-c'\n",
        "# DATASET = 's-r-5o'\n",
        "# DATASET = 's-r-5c'\n",
        "# DATASET = 's-r-10c'\n",
        "# DATASET = 'saa-march'\n",
        "\n",
        "# Models:\n",
        "MODEL = 'wavlm-basic'\n",
        "# MODEL = 'wavlm-tdnn'\n",
        "# MODEL = 'wav2vec2'\n",
        "# MODEL = 'tdnn'\n",
        "\n",
        "# Learning parameters:\n",
        "FREEZING = False\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 1e-4\n",
        "AUDIO_LENGTH_SEC = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KRhAbQSnPYW"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"reralle/\" + DATASET\n",
        "FREEZE_STRING = 'freezing' if FREEZING else 'unfrozen'\n",
        "OUTPUT_MODEL = MODEL + \"_\" + DATASET + \"_\" + str(BATCH_SIZE) + \"batch_\" + str(AUDIO_LENGTH_SEC) + \"sec_\" + str(LEARNING_RATE) + \"lr_\" + FREEZE_STRING\n",
        "OUTPUT_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN_u6l9JiCL6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/accelerate   ## as of may 11th something is broken in the pypi version so installing it via github\n",
        "!pip install transformers==4.28.0 datasets evaluate huggingface_hub torchaudio librosa \n",
        "!export PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtjOfxe_SonG"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EUdxdulRh57"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric, Audio\n",
        "from transformers import AutoFeatureExtractor, TrainingArguments, Trainer, EarlyStoppingCallback, IntervalStrategy\n",
        "from transformers.file_utils import ModelOutput\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchaudio\n",
        "import evaluate\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Audio as IPythonAudio\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from datetime import datetime\n",
        "from typing import Optional, Tuple\n",
        "from dataclasses import is_dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZiiQfh1zVgS"
      },
      "outputs": [],
      "source": [
        "if MODEL == 'wavlm-basic':\n",
        "  from transformers import WavLMForSequenceClassification\n",
        "  BASE_MODEL = \"microsoft/wavlm-large\"\n",
        "\n",
        "if MODEL == 'wavlm-tdnn':\n",
        "  from transformers import WavLMModel, WavLMPreTrainedModel\n",
        "  BASE_MODEL = \"microsoft/wavlm-large\"\n",
        "\n",
        "if MODEL == 'wav2vec2':\n",
        "  from transformers import Wav2Vec2ForSequenceClassification\n",
        "  BASE_MODEL = \"facebook/wav2vec2-large\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb7GLK7HknGm"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIpT-GHzQ8Pr"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(DATASET_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_JCHyUwR3eC"
      },
      "outputs": [],
      "source": [
        "labels = dataset[\"train\"].features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label\n",
        "num_labels = len(id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AeRg8EXp4RD"
      },
      "source": [
        "\n",
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))"
      ],
      "metadata": {
        "id": "1HPtdYdnCRSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VhtBGrKVfJ2"
      },
      "outputs": [],
      "source": [
        "if MODEL in {'wavlm-basic', 'wavlm-tdnn', 'wav2vec2'}:\n",
        "    \n",
        "    feature_extractor = AutoFeatureExtractor.from_pretrained(BASE_MODEL)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
        "        inputs = feature_extractor(\n",
        "            audio_arrays, \n",
        "            sampling_rate=feature_extractor.sampling_rate, \n",
        "            max_length=AUDIO_LENGTH_SEC * 16000, \n",
        "            truncation=True, \n",
        "        )\n",
        "        return inputs\n",
        "\n",
        "    encoded_dataset = dataset.map(preprocess_function, remove_columns=[\"audio\"], batched=True)\n",
        "    encoded_dataset\n",
        "\n",
        "elif MODEL == 'tdnn':\n",
        "    # For normalization\n",
        "    def compute_mean_std(dataset):\n",
        "        all_features = []\n",
        "\n",
        "        for idx in range(len(dataset)):\n",
        "            audio = dataset[idx][\"audio\"]\n",
        "            signal = audio[\"array\"][:(AUDIO_LENGTH_SEC * 16000)]\n",
        "            sr = audio[\"sampling_rate\"]\n",
        "            mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13, n_fft=int(0.025*sr), hop_length=int(0.010*sr))\n",
        "            mfccs = np.transpose(mfccs)\n",
        "            delta_mfccs = librosa.feature.delta(mfccs, order=1)\n",
        "            delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
        "            features = np.concatenate((mfccs, delta_mfccs, delta2_mfccs), axis=1)\n",
        "            all_features.append(features)\n",
        "\n",
        "        all_features = np.vstack(all_features)\n",
        "        mean = np.mean(all_features, axis=0)\n",
        "        std = np.std(all_features, axis=0)\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "    class MFCCDataset(Dataset):\n",
        "        def __init__(self, dataset, mean=None, std=None):\n",
        "            self.dataset = dataset\n",
        "            self.mean = mean\n",
        "            self.std = std\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.dataset)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            audio = self.dataset[idx]['audio']\n",
        "            label = self.dataset[idx]['label']\n",
        "\n",
        "            # Get the audio signal directly from the dataset\n",
        "            signal = audio['array'][:AUDIO_LENGTH_SEC * 16000]\n",
        "            sr = audio['sampling_rate']\n",
        "\n",
        "            # Compute MFCCs\n",
        "            mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13, n_fft=int(0.025*sr), hop_length=int(0.010*sr))\n",
        "\n",
        "            # Compute deltas and delta-deltas\n",
        "            mfccs_delta = librosa.feature.delta(mfccs)\n",
        "            mfccs_delta_delta = librosa.feature.delta(mfccs_delta)\n",
        "\n",
        "            # Concatenate MFCCs, deltas, and delta-deltas\n",
        "            features = torch.tensor(np.vstack([mfccs, mfccs_delta, mfccs_delta_delta]), dtype=torch.float32)\n",
        "\n",
        "            # Transpose\n",
        "            features = torch.transpose(features, 0, 1)\n",
        "\n",
        "            # Normalize the features\n",
        "            if self.mean is not None and self.std is not None:\n",
        "                features = (features - self.mean) / self.std\n",
        "\n",
        "            return features.float(), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    train_mean, train_std = compute_mean_std(dataset[\"train\"])\n",
        "\n",
        "    # Create the PyTorch Datasets\n",
        "    train_dataset = MFCCDataset(dataset[\"train\"], mean=train_mean, std=train_std)\n",
        "    validation_dataset = MFCCDataset(dataset[\"validation\"], mean=train_mean, std=train_std)\n",
        "    test_dataset = MFCCDataset(dataset['test'], mean=train_mean, std=train_std)\n",
        "\n",
        "    # Create the DataLoaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFzofsPd2tnU"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL in {'wavlm-tdnn', 'tdnn'}:\n",
        "\n",
        "  # Source: https://github.com/cvqluu/TDNN\n",
        "  class TDNN(nn.Module):\n",
        "    \n",
        "    def __init__(\n",
        "                    self, \n",
        "                    input_dim=23, \n",
        "                    output_dim=512,\n",
        "                    context_size=5,\n",
        "                    stride=1,\n",
        "                    dilation=1,\n",
        "                    batch_norm=False,\n",
        "                    dropout_p=0.0\n",
        "                ):\n",
        "        '''\n",
        "        TDNN as defined by https://www.danielpovey.com/files/2015_interspeech_multisplice.pdf\n",
        "        Affine transformation not applied globally to all frames but smaller windows with local context\n",
        "        batch_norm: True to include batch normalisation after the non linearity\n",
        "        \n",
        "        Context size and dilation determine the frames selected\n",
        "        (although context size is not really defined in the traditional sense)\n",
        "        For example:\n",
        "            context size 5 and dilation 1 is equivalent to [-2,-1,0,1,2]\n",
        "            context size 3 and dilation 2 is equivalent to [-2, 0, 2]\n",
        "            context size 1 and dilation 1 is equivalent to [0]\n",
        "        '''\n",
        "        super(TDNN, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.stride = stride\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dilation = dilation\n",
        "        self.dropout_p = dropout_p\n",
        "        self.batch_norm = batch_norm\n",
        "      \n",
        "        self.kernel = nn.Linear(input_dim*context_size, output_dim)\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "        if self.batch_norm:\n",
        "            self.bn = nn.BatchNorm1d(output_dim)\n",
        "        if self.dropout_p:\n",
        "            self.drop = nn.Dropout(p=self.dropout_p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        input: size (batch, seq_len, input_features)\n",
        "        outpu: size (batch, new_seq_len, output_features)\n",
        "        '''\n",
        "\n",
        "        #print(\"x.shape in the forward of a tdnn layer: \", x.shape)\n",
        "\n",
        "        _, _, d = x.shape\n",
        "        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(self.input_dim, d)\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Unfold input into smaller temporal contexts\n",
        "        x = F.unfold(\n",
        "                        x, \n",
        "                        (self.context_size, self.input_dim), \n",
        "                        stride=(1,self.input_dim), \n",
        "                        dilation=(self.dilation,1)\n",
        "                    )\n",
        "\n",
        "        # N, output_dim*context_size, new_t = x.shape\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.kernel(x)\n",
        "        x = self.nonlinearity(x)\n",
        "        \n",
        "        if self.dropout_p:\n",
        "            x = self.drop(x)\n",
        "\n",
        "        if self.batch_norm:\n",
        "            x = x.transpose(1,2)\n",
        "            x = self.bn(x)\n",
        "            x = x.transpose(1,2)\n",
        "\n",
        "        return x\n",
        "\n",
        "  # Source: https://github.com/r39ashmi/e2e_dialect/blob/main/classification/main_tdnn.py\n",
        "  class TDNNHead(nn.Module):\n",
        "\n",
        "      def __init__(self, config):\n",
        "          super().__init__()\n",
        "          self.frame1 = TDNN(input_dim=config.hidden_size, output_dim=128, context_size=5, dilation=2, dropout_p=0.5)\n",
        "          self.frame2 = TDNN(input_dim=128, output_dim=128, context_size=3, dilation=3, dropout_p=0.5)\n",
        "          self.frame3 = TDNN(input_dim=128, output_dim=128, context_size=3, dilation=4, dropout_p=0.5)\n",
        "          self.frame4 = TDNN(input_dim=128, output_dim=128, context_size=1, dilation=1, dropout_p=0.5)\n",
        "          self.frame5 = TDNN(input_dim=128, output_dim=375, context_size=1, dilation=1, dropout_p=0.5)\n",
        "          self.relu=nn.ReLU()\n",
        "          self.linear1=nn.Linear(375,375)\n",
        "          self.linear2=nn.Linear(375,150)\n",
        "          self.linear3=nn.Linear(150,10)\n",
        "          nfilters=40\n",
        "\n",
        "      def forward(self, features, **kwargs):\n",
        "          output1=self.frame1(features)\n",
        "          output1=self.frame2(output1)\n",
        "          output1=self.frame3(output1)\n",
        "          output1=self.frame4(output1)\n",
        "          output1=self.frame5(output1)\n",
        "          output1=output1.permute(0,2,1)\n",
        "          output1=torch.mean(output1,2)\n",
        "          output1=self.relu(self.linear1(output1))\n",
        "          output1=self.relu(self.linear2(output1))\n",
        "          output1=self.linear3(output1)\n",
        "          return F.log_softmax(output1,dim=-1)"
      ],
      "metadata": {
        "id": "DSA4U6kGDJZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL == 'wavlm-tdnn':\n",
        "\n",
        "  class WavLMwithTDNN(WavLMPreTrainedModel):\n",
        "      def __init__(self, config):\n",
        "          super().__init__(config)\n",
        "          self.num_labels = config.num_labels\n",
        "          self.pooling_mode = \"mean\"\n",
        "          self.config = config\n",
        "\n",
        "          self.wavlm = WavLMModel(config)\n",
        "          self.classifier = TDNNHead(config)\n",
        "\n",
        "          self.init_weights()\n",
        "\n",
        "      def freeze_base_model(self):\n",
        "          \"\"\"\n",
        "          Calling this function will disable the gradient computation for the base model so that its parameters will not\n",
        "          be updated during training. Only the classification head will be updated.\n",
        "          \"\"\"\n",
        "          for param in self.wavlm.parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "\n",
        "      def forward(\n",
        "              self,\n",
        "              input_values,\n",
        "              attention_mask=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              return_dict=None,\n",
        "              labels=None,\n",
        "      ):\n",
        "          return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "          outputs = self.wavlm(\n",
        "              input_values,\n",
        "              attention_mask=attention_mask,\n",
        "              output_attentions=output_attentions,\n",
        "              output_hidden_states=output_hidden_states,\n",
        "              return_dict=return_dict,\n",
        "          )\n",
        "          hidden_states = outputs[0]\n",
        "          logits = self.classifier(hidden_states)\n",
        "          loss_fct = CrossEntropyLoss()\n",
        "          loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "          #print(f\"logits shape: {logits.shape}, labels shape: {labels.shape if labels is not None else None}\")\n",
        "\n",
        "          return SequenceClassifierOutput(\n",
        "              loss=loss,\n",
        "              logits=logits,\n",
        "              hidden_states=outputs.hidden_states,\n",
        "              attentions=outputs.attentions,\n",
        "          )"
      ],
      "metadata": {
        "id": "F35HAI5ODfqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpwyBjzL2T06"
      },
      "outputs": [],
      "source": [
        "if MODEL == 'wavlm-basic':\n",
        "  model = WavLMForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=num_labels, label2id=label2id, id2label=id2label)\n",
        "\n",
        "if MODEL == 'wavlm-tdnn':\n",
        "  model = WavLMwithTDNN.from_pretrained(BASE_MODEL, num_labels=num_labels, label2id=label2id, id2label=id2label)\n",
        "\n",
        "if MODEL == 'wav2vec2':\n",
        "  model = Wav2Vec2ForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=num_labels, label2id=label2id,mid2label=id2label)\n",
        "  \n",
        "if MODEL == 'tdnn':\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = TDNNHead(batch_size=BATCH_SIZE, in_channels=39, learnable=True)\n",
        "  model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xymfln-miN60"
      },
      "outputs": [],
      "source": [
        "if FREEZING:\n",
        "  model.freeze_base_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8b-quSx2xZP"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJuXyiA6be0P"
      },
      "outputs": [],
      "source": [
        "def print_metrics(accuracy, f1_average, acc_per_class, f1_per_class, cm, export):\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1 average: {f1_average}\")\n",
        "    print(\"Accuracy per class:\")\n",
        "    for i, label in id2label.items():\n",
        "        print(f\"\\t{label}: {acc_per_class[int(i)]}\")\n",
        "\n",
        "    print(\"F1 score per class:\")\n",
        "    for i, label in id2label.items():\n",
        "        print(f\"\\t{label}: {f1_per_class[int(i)]}\")\n",
        "\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(cm)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    if export:\n",
        "      current_dateTime = datetime.now()\n",
        "      f1_df = pd.DataFrame(f1_per_class)\n",
        "      f1_df.to_excel(f\"f1_epoch{current_dateTime}.xlsx\")\n",
        "      cmtx = pd.DataFrame(cm, index=['true:{:}'.format(x) for x in dataset[\"test\"].features[\"label\"].names], columns=['pred:{:}'.format(x) for x in dataset[\"test\"].features[\"label\"].names])\n",
        "      cmtx.to_excel(f\"cm_epoch{current_dateTime}.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St6vFtsEgrMX"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    true_labels = [id2label[str(x)] for x in eval_pred.label_ids]\n",
        "    predicted_labels = [id2label[str(x)] for x in predictions]\n",
        "  \n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    f1_per_class = f1_score(true_labels, predicted_labels, average=None)\n",
        "    f1_average = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    acc_per_class = cm.diagonal() / cm.sum(axis=1)\n",
        "\n",
        "    print_metrics(accuracy, f1_average, acc_per_class, f1_per_class, cm, export=False)\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1_average}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_nPOfzT3w0G"
      },
      "outputs": [],
      "source": [
        "# My twist on the early stopping callback, because the original did not work\n",
        "class MyEarlyStoppingCallback(EarlyStoppingCallback):\n",
        "\n",
        "    def __init__(self, early_stopping_patience: int = 1, early_stopping_threshold: Optional[float] = 0.0):\n",
        "        super().__init__(early_stopping_patience, early_stopping_threshold)\n",
        "        self.best_metric = 0.0\n",
        "\n",
        "    def check_metric_value(self, args, state, control, metric_value):\n",
        "\n",
        "        print(f\"best model: {trainer.state.best_model_checkpoint}\")\n",
        "        print(f\"metric value: {metric_value}\")\n",
        "\n",
        "        if metric_value - self.best_metric > self.early_stopping_threshold:\n",
        "            self.early_stopping_patience_counter = 0\n",
        "            print(\"reset early stopping patience\")\n",
        "        else:\n",
        "            self.early_stopping_patience_counter += 1\n",
        "            print(f\"increased early stopping counter to {self.early_stopping_patience_counter}\")\n",
        "\n",
        "        if metric_value > self.best_metric:\n",
        "            self.best_metric = metric_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0mkgu4jcWiQ"
      },
      "outputs": [],
      "source": [
        "# # Evaluate 3 times per epoch\n",
        "# # Useful for larger datasets but then this needs to be added to the training arguments:\n",
        "#     # evaluation_strategy = IntervalStrategy.STEPS, # \"steps\"\n",
        "#     # eval_steps = NUM_STEPS, # Evaluation and Save happens every n steps\n",
        "#     # save_steps = NUM_STEPS,\n",
        "#     #save_strategy = IntervalStrategy.STEPS,\n",
        "# NUM_STEPS = int((len(encoded_dataset[\"train\"]) * 31) / (3 * 1000))\n",
        "# NUM_STEPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9VPJn9bgrJj"
      },
      "outputs": [],
      "source": [
        "if MODEL in {'wavlm-basic', 'wavlm-tdnn', 'wav2vec2'}:\n",
        "  args = TrainingArguments(\n",
        "      OUTPUT_MODEL,\n",
        "      save_total_limit = 3,\n",
        "      evaluation_strategy =\"epoch\",\n",
        "      save_strategy=\"epoch\",\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      per_device_train_batch_size=BATCH_SIZE,\n",
        "      gradient_accumulation_steps=4,\n",
        "      per_device_eval_batch_size=BATCH_SIZE,\n",
        "      num_train_epochs=1000,\n",
        "      warmup_ratio=0.003,\n",
        "      logging_steps=15,\n",
        "      load_best_model_at_end=True, \n",
        "      metric_for_best_model = 'eval_f1',\n",
        "      push_to_hub=True,\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model,\n",
        "      args,\n",
        "      train_dataset=encoded_dataset[\"train\"],\n",
        "      eval_dataset=encoded_dataset[\"validation\"],\n",
        "      tokenizer=feature_extractor,\n",
        "      compute_metrics=compute_metrics,\n",
        "      callbacks = [MyEarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.005)]\n",
        "  )\n",
        "\n",
        "  trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL in {'wavlm-basic', 'wavlm-tdnn', 'wav2vec2'}:\n",
        "  trainer.evaluate(eval_dataset=encoded_dataset[\"test\"])"
      ],
      "metadata": {
        "id": "noqPEa61F64N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq6PK0Wnz4OQ"
      },
      "outputs": [],
      "source": [
        "if MODEL in {'wavlm-basic', 'wavlm-tdnn', 'wav2vec2'}:\n",
        "  trainer.save_model()\n",
        "  trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL == 'tdnn':\n",
        "    # Set loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    # Training function\n",
        "    def train(model, dataloader, criterion, optimizer, device):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for features, labels in dataloader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * features.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            #print(f\"{loss=} {running_loss=} {outputs.max(1)=} {labels.size(0)=} {total=} {correct=}\")\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    # Validation function\n",
        "    def validate(model, dataloader, criterion, device):\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for features, labels in dataloader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * features.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 30\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train(model, train_dataloader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = validate(model, validation_dataloader, criterion, device)\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        print('Epoch: {}/{}, Train Loss: {:.4f}, Train Acc: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}'.format(\n",
        "            epoch + 1, num_epochs, train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "        # Save the best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f\"{OUTPUT_MODEL}_best.pth\")\n",
        "\n",
        "    print(\"Training completed. The best model is saved as\", f\"{OUTPUT_MODEL}_best.pth\")\n",
        "\n",
        "    # Plot losses\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label=\"Training Loss\")\n",
        "    plt.plot(val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.figure()\n",
        "    plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
        "    plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TePqyqv_G5wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL == 'tdnn':\n",
        "    # Load the best model and test\n",
        "    model.load_state_dict(torch.load(f\"{OUTPUT_MODEL}_best.pth\"))\n",
        "    test_loss, test_acc = validate(model, test_dataloader, criterion, device)\n",
        "    print(\"Test Loss: {:.4f}, Test Acc: {:.4f}\".format(test_loss, test_acc))"
      ],
      "metadata": {
        "id": "AvuuelAfG6Vw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}